<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Blog Posts</title>

  
  <meta name="author" content="Some Person">
  

  <meta name="description" content="This is what I write about">

  

  

  
  <link rel="alternate" type="application/rss+xml" title="Elad Rapaport" href="/feed.xml">
  

  

  

  

  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Elad Rapaport">
  <meta property="og:title" content="Blog Posts">
  <meta property="og:description" content="This is what I write about">

  
  <meta property="og:image" content="/assets/img/avatar-icon.png">
  

  
  <meta property="og:type" content="website">
  <meta property="og:url" content="/blogposts/">
  <link rel="canonical" href="/blogposts/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@daattali">
  <meta name="twitter:creator" content="@daattali">

  <meta property="twitter:title" content="Blog Posts">
  <meta property="twitter:description" content="This is what I write about">

  
  <meta name="twitter:image" content="/assets/img/avatar-icon.png">
  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="">Elad Rapaport</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/index">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/aboutme">About Me</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="/blogposts">Blog Posts</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  
    <div class="avatar-container">
      <div class="avatar-img-border">
        <a href="">
          <img alt="Navigation bar avatar" class="avatar-img" src="/assets/img/avatar-icon.png" />
        </a>
      </div>
    </div>
  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Sample blog post", \
          "category" : "test", \
          "url"      : "/2020-02-28-test-markdown/", \
          "date"     : "February 28, 2020" \
        }, \
       \
        { \
          "title"    : "Flake it till you make it", \
          "category" : "bookstest", \
          "url"      : "/2020-02-26-flake-it-till-you-make-it/", \
          "date"     : "February 26, 2020" \
        }, \
       \
       \
        { \
          "title"    : "About me", \
          "category" : "page", \
          "url"      : "/aboutme/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Blog Posts", \
          "category" : "page", \
          "url"      : "/blogposts/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Elad Rapaport", \
          "category" : "page", \
          "url"      : "/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Tag Index", \
          "category" : "page", \
          "url"      : "/tags/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="page-heading">
          <h1>Blog Posts</h1>
          
            
              <hr class="small">
              <span class="page-subheading">This is what I write about</span>
            
          

          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md " role="main">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">
      

      <section id="main" class="wrapper style1">
  <header class="major">
      <h2>Blog</h2>
  </header>
  
    <h2>Build a Snake Game Using Go</h2>
    <p><h4>Play your way to learning Go — step-by-step</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*NZqKf6ClGmHh_aOO" /><figcaption>Photo by <a href="https://unsplash.com/@carltraw?utm_source=medium&amp;utm_medium=referral">Carl Raw</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>In this article, we are going to build together the classic game of Snake using Go. This is my first project in Go and what better opportunity to share my learning experience with you and hopefully help your journey to learning Go as well?</p><p>The full code for this project (separated into parts) is in this <a href="https://github.com/erap129/SnakeInGo">GitHub repo</a></p><p>The outline of this post is as follows:</p><ul><li>Part 1— Why Bother?</li><li>Part 2— Setting up our environment</li><li>Part 3— Implementing Snake in Go<br>- Part 3.1— A moving blob<br>- Part 3.2— A moving snake<br>- Part 3.3— Adding food and points</li></ul><h3><strong>Part 1— Why Bother?</strong></h3><p>Why learn a new language? And why not just follow the official tutorial and call it a day?</p><ul><li>Learning a new language makes us better programmers. Every programming language has its set of strengths and features and opening up to languages we haven’t seen before can open us to new ideas and programming paradigms and also add another tool to our toolchain.</li><li>What better way to learn than by games? I personally think it’s way more fun than following an instruction manual.</li></ul><p>So why Go?</p><p>Go is a language created by Google, with version 1.0 being released to the public back in 2012. Since then Go has developed and gained major traction amongst developers. Go is notable for being fast and easy to learn with a strong focus on efficient concurrency (read about <a href="https://golangbot.com/goroutines/">goroutines</a> [2] for more info). I decided that such a popular and trending language deserves a bit of my attention, even if I’m not planning to use it for anything practical in the near future, and thus this tutorial was born!</p><h3>Part 2— Setting up our environment</h3><p>The installation part is relevant only for Linux. Note that it will install an older version of Go (1.13 as of writing this article) but it is sufficient for our project.</p><p><strong>Installation:</strong> Install Go using the following command: sudo apt install golang-go . After following through the installation, check that your system has installed Go by running go version . Your output should look something like this: go version go1.13.8 linux/amd64 (depending on the version you installed). After this is done we can create our first project!</p><p><strong>Creating a project: </strong>We will be using Visual Studio Code (vscode) as our IDE (you can still follow through using other IDEs). Create a new folder for our project called Snake and open up the IDE in this folder. If using vscode you should install the Go extension via the Extensions tab.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/384/1*LeIVvFCosyiH88sD9BxBOg.png" /><figcaption>vscode — Go extension</figcaption></figure><p>Open up the terminal (can be done directly in VS Code), type go mod init Snake , and hit Enter.</p><p>This should create a new file go.mod in your current directory (the Snake directory). If we were publishing this project as a module we would have replaced Snake with a valid URL, such as our GitHub repository, but since this is just a toy project we don’t care about that.</p><p>Let’s just write a simple “Hello World” project to see that everything is up and running properly. Create a new file called main.go in the Snake directory and type in the following code:</p><pre>package main</pre><pre>import &quot;fmt&quot;</pre><pre>func main() {</pre><pre>    fmt.Println(&quot;Hello World!&quot;)</pre><pre>}</pre><p>In this code, we imported and used the standard fmt package which deals with command line input and output. In the terminal run go run main.go. You should see Hello World! printed in your terminal and the program should exit. If this is done we are good to go to the next part — building a Snake game!</p><p>When you want to run a project consisting of several files (as in this game) — from the directory that contains your code run <strong>go run *.go</strong> . What will happen is that your main function will run with awareness of the other files in the project (if you just run go run main.go it will ignore all of the other files and break due to missing dependencies).</p><h3>Part 3— Implementing Snake in Go</h3><p>This implementation is inspired by and based on the <a href="https://earthly.dev/blog/pongo/">Pong Go tutorial</a> [3] created by Josh Aletto.</p><h4><strong>Part 3.1— A moving blob</strong></h4><p>In this first part, we will create a keyboard-controlled blob on the screen. We are starting with this because it is a relatively simple logic task and it will allow us to focus on the basics of the language and the terminal-based GUI framework we will be using — <a href="https://github.com/gdamore/tcell">tcell</a> [4].</p><p>Part I will consist of three files —</p><ul><li>main.go will contain the UI loop which will listen for user input and update the relevant data structures accordingly. It will also initiate the subroutine in game.go.</li><li>game.go deals with the main game loop — updating the state of the different elements in the game and drawing them on the terminal screen.</li><li>snake.go handles the inner state of the snake object (currently just its speed and position)</li></ul><p>Let’s view the files:</p><p>main.go —</p><ul><li>In lines 11–18 we get the screen object from tcell and do some error handling when screen fetching fails.</li><li>In lines 23–33 we define our two objects, the game and the SnakeBody and initialize them with some initial parameters.</li><li>In line 34 we run a goroutine which starts the main loop of the game in a different thread. We will view the game loop later.</li><li>In lines 35–53 we deal with user input. Either the user presses ctrl+c and then we exit the program, or the user presses one of the arrow keys and then we send the relevant signal to the game.SnakeBody object, to change the direction of movement.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/baed95f3a34eb1e01add84bb30d288f2/href">https://medium.com/media/baed95f3a34eb1e01add84bb30d288f2/href</a></iframe><p>game.go—</p><ul><li>Lines 21–27 define the main game loop. At the start of each step we clear the screen, then we update the snakeBody location and draw it on the screen. After that, we perform time.Sleep to hold the previous frame on the screen (otherwise the movement would be too fast for us to visually perceive). Finally, we draw the new frame onto the screen.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e904c3b9914ce0152573ae968f6ac554/href">https://medium.com/media/e904c3b9914ce0152573ae968f6ac554/href</a></iframe><p>snake.go —</p><ul><li>This file defines our snake object (currently just a blob) and deals with the changing of direction and the updating of the current location using the current speed. Notice that if the snake disappears into one edge of the screen we want it to reappear at the other edge, hence the complexity in lines 16–23.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/223d4b5b98db88b9a24bcb8c495aa5de/href">https://medium.com/media/223d4b5b98db88b9a24bcb8c495aa5de/href</a></iframe><p>The gameplay in this part should look like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/780/1*tLvO4hv3mkkOBR0PlUOVzg.gif" /><figcaption>Gameplay — part 1</figcaption></figure><h4><strong>Part 3.3— A moving snake</strong></h4><p>To implement a moving snake we will first need to define what being a “snake” means. In this context it means an entity consisting of several parts, where each part has its own X and Y coordinates. By noticing that the movement of the snake can be modelled using a queue we can implement this in a simple and elegant way, as shown in the following figure.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1016/1*Gx8FnZX0JZVxyjoZqT2gZA.png" /><figcaption>Snake movement implementation as a queue</figcaption></figure><p>Most of our changes will occur in the file snake.go where the queue logic will be implemented (Go doesn’t have built-in queues so we will implement them using slices). Some minor changes will also happen in main.go and game.gomainly to support the changes in the former.</p><p>main.go —</p><ul><li>In lines 23–36 added an initialization to the snakeParts object which constitutes the initial building blocks of the snake.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0e65ff9ac5473334d2e5128504e50087/href">https://medium.com/media/0e65ff9ac5473334d2e5128504e50087/href</a></iframe><p>game.go —</p><ul><li>Added the drawParts function which will draw the snake’s parts onto the screen in each iteration of the game loop.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/9b5a5d4734988ec8d21a4ebf909ba045/href">https://medium.com/media/9b5a5d4734988ec8d21a4ebf909ba045/href</a></iframe><p>snake.go—</p><ul><li>Added the Update function which implements the snake’s movement in a queue-like fashion. In each activation of this function a new head is enqueued (depending on the direction of the snake) and the tail’s end is dequeued.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/163f74371a84aa0039d78f7610b3bd31/href">https://medium.com/media/163f74371a84aa0039d78f7610b3bd31/href</a></iframe><p>The gameplay in this part should look like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/780/1*Xz61T_y1iLMkbObe6q7VZg.gif" /><figcaption>Gameplay — part 2</figcaption></figure><h4><strong>Part 3.3— Adding food and points</strong></h4><p>In this part, we will add important logic to the game including:</p><ul><li>The ability to collect food and earn points</li><li>Make the snake longer if it collected food</li><li>Detecting collisions of the snake’s head with its body</li><li>A “Game Over” screen, with the option to replay or to quit</li></ul><p>Let’s discuss the changes required in each of the 3 files:</p><p>snake.go —</p><ul><li>Lines 21–23: check if the longerSnake parameter was passed as true. If so, Don’t perform a dequeue on the snake’s body and thus make it one tile longer.</li><li>Added the ResetPos function for convenience (this function will be called from main.go)</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e40c611cbc648f1a1d60b82ac76d4cc3/href">https://medium.com/media/e40c611cbc648f1a1d60b82ac76d4cc3/href</a></iframe><p>game.go—</p><ul><li>Added drawText function to draw text on the screen. Taken from <a href="https://github.com/liweiyi88/gosnakego">another snake implementation using cell</a> [5].</li><li>Added checkCollision function which receives a List of parts and another part. For each part in the list of parts, it checks whether it collides with the other part. We use this function to check 1) whether the snakehead has collided with food and 2) whether the snakehead has collided with the snake body.</li><li>Added logic in the main loop (I will detail the less trivial additions):<br>1) lines 72–76: check for collision with food and update score if collision detected. Also, redraw food in a random location.<br>2) lines 86–88: Game Over screen. Notice that after displaying the score this subroutine will terminate (In main.go the user will have an option to start another goroutine with a fresh game)</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/240472b32dc753b5e03d44dcac97dd93/href">https://medium.com/media/240472b32dc753b5e03d44dcac97dd93/href</a></iframe><p>main.go —</p><ul><li>Lines 43–47: These lines deal with the Game Over situation. If a game over is detected (using the GameOver bool in game), the user has an option to press y or n and the game will either restart or terminate accordingly. We restart the game simply by initiating another goroutine.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6366550700086ad27035c763e14be0b5/href">https://medium.com/media/6366550700086ad27035c763e14be0b5/href</a></iframe><p>The gameplay in this part should look like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/780/1*p3yyUi8OQ3KX2i1F7Dtljg.gif" /><figcaption>Gameplay — part 3</figcaption></figure><p>And we’re done with the implementation! Your snake game is up and running and you can invite friends over for a tournament :)</p><h3>Thoughts Ahead</h3><p>I hope you had fun implementing Snake in Go, I know I did. I think goroutines are a super-cool feature, I don’t know other languages where it is that easy to perform parallelization. Although this game is fairly simple I still liked the queue implementation of movement as a small mental exercise. This experience will also shorten the learning curve if I ever have to use this language for anything practical, and it’s always good to have another tool under my belt.</p><p>If I had more time I would extend this game and add cool power-ups to my snakes such as particle-shooting abilities, speed changing, and other goodies. I would also add unit tests to make sure that nothing breaks after these feature additions.</p><p>Thanks for reading, see you next time!</p><h3>References</h3><p>[1] — Full project code: <a href="https://github.com/erap129/SnakeInGo">https://github.com/erap129/SnakeInGo</a></p><p>[2] —goroutines: <a href="https://golangbot.com/goroutines/">https://golangbot.com/goroutines/</a></p><p>[3] — Pong go tutorial: <a href="https://earthly.dev/blog/pongo/">https://earthly.dev/blog/pongo/</a></p><p>[4] — tcell Go package: <a href="https://github.com/gdamore/tcell">https://github.com/gdamore/tcell</a></p><p>[5] — Another snake implementation using tcell: <a href="https://github.com/liweiyi88/gosnakego">https://github.com/liweiyi88/gosnakego</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b4186e21d011" width="1" height="1" alt=""><hr><p><a href="https://betterprogramming.pub/build-a-snake-game-using-in-go-b4186e21d011">Build a Snake Game Using Go</a> was originally published in <a href="https://betterprogramming.pub">Better Programming</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
  
    <h2>MovieLens-1M Deep Dive — Part II,  Tensorflow Recommenders</h2>
    <p><h3>MovieLens-1M Deep Dive — Part II, Tensorflow Recommenders</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JbAFlnuM2VOBZt0tKjf5FA.jpeg" /><figcaption>Photo by Nathan Engel: <a href="https://www.pexels.com/photo/time-lapse-photography-of-car-lights-in-front-of-cinema-436413/">https://www.pexels.com/photo/time-lapse-photography-of-car-lights-in-front-of-cinema-436413/</a></figcaption></figure><p>Hello readers,</p><p>For those of you who haven’t read the previous part, here is the link:</p><p><a href="https://towardsdatascience.com/movielens-1m-deep-dive-part-i-8acfeda1ad4">MovieLens-1M Deep Dive — Part I</a></p><p>In that article, I present the MovieLens-1M [1] dataset (a movie recommendations dataset that contains 1 million ratings for movies made by different users) along with some exploratory data analysis and try out some classical recommender systems algorithms. Although, that article is not a pre-requisite and you will be able to understand the content here without reading it.</p><p>In the second installment of the MovieLens deep dive series, we will use TensorFlow recommenders (TFRS). TFRS will allow us to abstract away much of the gory details involved when dealing with recommender systems, such as evaluation metrics and the model training procedure. Our model will be based on the two-tower architecture, which we will review before delving into the code.</p><p>In the coding sections, I will somewhat extend an existing official TFRS tutorial — <a href="https://www.tensorflow.org/recommenders/examples/deep_recommenders">Building Deep Retrieval Models</a> [3]. In this tutorial, the goal is to build a retrieval recommender system using rich context features of the items (movies) and the users. We will test the impact of adding different features and try to gain improvements by introducing smarter and more relevant features to the task at hand.</p><p>The outline of the article is as follows:</p><ol><li>A brief overview of modern recommender system methods</li><li>Get to know TFRS and the retrieval task — the basic building blocks which we will be using</li><li>Beat the benchmark — attempt to outperform the original TFRS tutorial model by using more contextual features</li><li>Beat the benchmark — results</li><li>Conclusion and thoughts ahead</li></ol><p>Let’s dive in!</p><h3>Part I — A Brief Overview of Modern Recommender System Methods</h3><p>The recommender systems pipeline is usually split into two stages — retrieval and ranking. In the retrieval stage, we generate representations for items and users in our system (usually in the form of embeddings) and select a subset of items that the user might be interested in. This stage may deal with millions of users/items and thus must be computationally efficient. In the ranking stage, we take the output of the retrieval stage and rank them to select the final few items that will be suggested to the user. This model may be more computationally intensive as it must deal with far fewer data. A depiction of this process is shown in figure 1.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eDYJDXmV7VKSN5yBUyuHVw.png" /><figcaption>Fig. 1: The recommender systems process</figcaption></figure><p>In this walkthrough, we will focus on the retrieval stage, and try to make it more efficient and accurate by taking into account different contextual features in the embedding generation process.</p><p>Now for some terminology:</p><ul><li>Items — These are the items that we will recommend to users. In our case — movies.</li><li>Queries — This is the information our system will use to make item recommendations. In our case, this will include the ID of the user we are recommending and other features such as the age and gender of the user. We will also include it here</li></ul><h3>Part II — Get to know TFRS and the retrieval task</h3><p>Enter TFRS, a recommender systems library built on top of Tensorflow, intended on streamlining the process of building, training, and deploying recommender systems.</p><p>The models we build in TFRS will inherit from the <a href="https://www.tensorflow.org/recommenders/api_docs/python/tfrs/models/Model">tfrs. model.Model</a> class. This class is itself a wrapper around tf.keras.Modeland saves us the hassle of writing the train_step and test_step methods. What we as users have to do is write the compute_loss function, which receives a batch of input queries and items and computes the loss for the model.</p><p>In our tutorial, we will build a two-tower architecture model. This model contains one tower for the items and one tower for the queries. As you can see in figure 2, the two-tower architecture is pretty simple. Embeddings are created to represent the queries and the users, and eventually, we perform a cosine similarity operation on the two embedding vectors to predict their affinity (is the query a good match for this item?). By sorting the results of a certain query using the cosine similarity with each item embedding the system can retrieve the top items for this user (usually in the order of 1000&#39;s), which will be then most likely fed into a ranking model for fine-grained control over the final items that will be presented to the user (usually up to 10 items).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XQ565jpYkqeGbw_vjfl3hw.png" /><figcaption>Fig. 2: Simple illustration of the two-tower architecture</figcaption></figure><p>What we don’t understand from the explanation above though is, how does the model learn? Well, of course by gradient descent. There are positive examples (in our case, movies that were rated by a certain user) and negative examples (movies that were not rated by a certain user). Our algorithm will treat this as a multiclass classification task and try to maximize the similarity between the rated items and the query and minimize the similarity between non-rated items and the query. This operation is done per batch (each batch consists of only positive examples, and the negative examples are generated by pairing the different queries and items in the batch, as shown in figure 3).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nSEuImpGi8JfgPn4juNyLw.png" /><figcaption>Fig. 3: The retrieval task (FFN=feedforward network)</figcaption></figure><h3>Part III — Beat the benchmark</h3><p>In this part, I will present my code to you and we will walk through the process of trying to improve the performance of our deep retrieval model, by enriching the item and query representations using additional context features.</p><p>Let’s review the features we will use for both items and queries. For each feature, I will specify if it was used in the original TFRS tutorial or not.</p><p><strong>Query features (user + context info):</strong></p><ul><li>user_id [int] — the most basic feature. Used to index into the user embedding matrix. (Used in original tutorial)</li><li>timestamp [int] — The time of the query performed by the user, represented in UNIX timestamp format. This can help us capture seasonal trends in the data (for example — on weekends people might prefer longer movies). (Used in original tutorial)</li><li>user_age [int] — An additional numeric feature that might hint into patterns in user preferences. (Not used in original tutorial)</li></ul><p><strong>Item features (movie info):</strong></p><ul><li>movie_title [string] — A unique identifier for items. Used to index into the item embedding matrix. This feature will also be used to create a sentence embedding of the movie title text (as opposed to just using it as a plain identifier). (Used in original tutorial)</li><li>movie_genres [list&lt;int&gt;] — Each movie is associated with one or more genres, where each genre is defined by a unique integer. We will use these integers to index into a genre embedding matrix. For each movie, we will average the embeddings for all of its genres. (Not used in original tutorial)</li><li>movie_length [int] — The length of the movie in minutes. This info was collected using the <a href="https://cinemagoer.github.io/">Cinemagoer</a>[2] python package. Some of the movie length data was missing and for these movies, I gave a default length of 90 minutes.</li></ul><p><strong>The code:</strong></p><p>I present here a non-conventional way of doing exploratory data science work, and that is via scripting and not via Jupyter notebooks. I think the advantages of scripting VS notebooks are:</p><p>1) Cleaner code. Encourages you to use good programming practices, such as extracting common procedures into functions instead of copy-pasting between cells.</p><p>2) The ability to track your work on it in a sane way, which is of utmost importance, especially when collaborating with other people.</p><p>The downside is that it might introduce the need for caching mechanisms, as I have done in this case (so you won’t have to run a heavy data loading/model training process for each experiment you wish to perform). In this example, I save the trained models for each configuration so I won’t have to re-run training every time I run the script. Additionally, I save and load the dataset using tf.data.Dataset.save and tf.data.Dataset.load because the dataset creation process can take a while. Finally, I utilize the <a href="https://pypi.org/project/cachier/">cachier</a> python package, which generates persistent caching of function results on disk, when loading the movie length information from Cinemagoer as this is a very lengthy process and we want to avoid running this function more than once for each movie.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4b2f8c771087dc9526d16fc3ab9f7d9a/href">https://medium.com/media/4b2f8c771087dc9526d16fc3ab9f7d9a/href</a></iframe><p>The required non-standard packages to run this script are as follows: tensorflow, tensorflow_datasets, tensorflow_recommenders, plotly, numpy . Note that the script assumes you have two sub-directories in your main directory named datasets and saved_models.</p><p>The training process will be managed by the MovieLensTrainer class. Given training settings such as layer sizes, embedding size, and which features to use, it will either train models or load the relevant models from the disk. After obtaining the trained models for all of the feature combinations we defined via the command-line arguments, we will run the function plot_training_runs which will return a plotly line chart of the top-100 categorical accuracy of each run and will allow us to compare the different settings we used.</p><p>An important change I made to the model from the original TFRS tutorial is the ability to use additional features in the MovieModel (the item tower). In the tutorial, this model takes just the movie_title as input. In this project it can take any additional feature, eventually creating a concatenation of the embeddings of all features. This is important as it enables us to leverage additional movie information such as the genres, movie length, average rating on IMDB, and any other feature you may think of.</p><h4>Command line arguments + Run example</h4><p>We have several command-line arguments that control what our script does. I will go over the less obvious ones:</p><p>layer_sizes (list[int]) — This defines the number and size of the layers following the embedding layer for each of the two towers (query and item models). The more layers we add and the more neurons we give them, the more complex our model will become (and thus more prone to overfitting as well)</p><p>additional_feature_sets (list[list[string]]) — This argument is a list of lists, where each inner list is a set of features we wish to train a TFRS model on. For example, using this argument we can train two models, where one trains using the movie_genres, timestamp features, and the other trains using movie_title_text, timestamp features.</p><p>--retrain (bool) — This is a binary toggle. When off, we will load a previously trained model with the selected settings if we already trained before (loaded from the file system). If on, we will always re-train and overwrite the models currently on the disk.</p><p>--generate_recommendations_for_user (int) — This is an ID of a user for which we will generate recommendations for each trained model that we produced. This is meant to bring our model “down to earth” and see real examples of the recommendations it generates for a user whose preferences are known.</p><p>Example of a set of command line arguments for this process (this run will train 6 different models, each for 300 epochs. It will generate recommendations for a user with an ID of 1 for each trained model):</p><pre>--num_epochs<br>300<br>--additional_feature_sets<br>None<br>--additional_feature_sets<br>timestamp<br>--additional_feature_sets<br>movie_title_text<br>--additional_feature_sets<br>timestamp<br>movie_title_text<br>--additional_feature_sets<br>timestamp<br>movie_title_text<br>movie_genres<br>--additional_feature_sets<br>timestamp<br>movie_title_text<br>movie_genres<br>movie_length<br>--embedding_size<br>32<br>--layer_sizes<br>64<br>32<br>--generate_recommendations_for_user<br>1</pre><p>Let’s view the output of this run (removed warnings for clarity):</p><p>These are the ratings for user 1:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a6fc7d2e8d577ab1373f7c846997f8da/href">https://medium.com/media/a6fc7d2e8d577ab1373f7c846997f8da/href</a></iframe><pre><br><strong>Recommendations for model (&#39;timestamp&#39;,), user 1:</strong><br> [b&#39;Alien (1979)&#39; b&#39;Sense and Sensibility (1995)&#39; b&#39;Sea Wolves, The (1980)&#39;<br> b&#39;Ninth Gate, The (2000)&#39; b&#39;Beautiful (2000)&#39; b&#39;Sweet Nothing (1995)&#39;<br> b&#39;Titan A.E. (2000)&#39; b&#39;Saint, The (1997)&#39;<br> b&#39;Killer: A Journal of Murder (1995)&#39; b&#39;Man of the Year (1995)&#39;]<br>Recommendations for model (&#39;movie_title_text&#39;,), user 1:<br> [b&#39;Angel and the Badman (1947)&#39; b&#39;Alien (1979)&#39; b&#39;Shadows (Cienie) (1988)&#39;<br> b&#39;Two Moon Juction (1988)&#39; b&#39;Sea Wolves, The (1980)&#39; b&#39;Booty Call (1997)&#39;<br> b&#39;Apple Dumpling Gang Rides Again, The (1979)&#39;<br> b&#39;Psycho Beach Party (2000)&#39; b&#39;Local Hero (1983)&#39;<br> b&quot;Shaft&#39;s Big Score! (1972)&quot;]<br><strong>Recommendations for model (&#39;timestamp&#39;, &#39;movie_title_text&#39;), user 1:</strong><br> [b&#39;Fried Green Tomatoes (1991)&#39; b&#39;Alien (1979)&#39;<br> b&#39;Angel and the Badman (1947)&#39; b&#39;Sarafina! (1992)&#39;<br> b&#39;Color Purple, The (1985)&#39; b&#39;Sea Wolves, The (1980)&#39;<br> b&#39;Madame Sousatzka (1988)&#39; b&#39;Little Shop of Horrors (1986)&#39;<br> b&quot;One Flew Over the Cuckoo&#39;s Nest (1975)&quot; b&#39;L.A. Story (1991)&#39;]<br><strong>Recommendations for model (&#39;timestamp&#39;, &#39;movie_title_text&#39;, &#39;movie_genres&#39;), user 1:<br></strong> [b&#39;Being There (1979)&#39; b&#39;Alien (1979)&#39; b&#39;Crimes and Misdemeanors (1989)&#39;<br> b&#39;Two Moon Juction (1988)&#39; b&#39;Psycho (1998)&#39;<br> b&#39;Apple Dumpling Gang Rides Again, The (1979)&#39; b&#39;Me Myself I (2000)&#39;<br> b&#39;Loser (2000)&#39; b&#39;Killer: A Journal of Murder (1995)&#39;<br> b&quot;Barney&#39;s Great Adventure (1998)&quot;]<br><strong>Recommendations for model (&#39;timestamp&#39;, &#39;movie_title_text&#39;, &#39;movie_genres&#39;, &#39;movie_length&#39;), user 1:<br></strong> [b&#39;Sid and Nancy (1986)&#39; b&#39;Angel and the Badman (1947)&#39; b&#39;Alien (1979)&#39;<br> b&#39;Two Moon Juction (1988)&#39; b&#39;Crimes of the Heart (1986)&#39;<br> b&#39;Sea Wolves, The (1980)&#39; b&#39;One False Move (1991)&#39;<br> b&#39;Rich and Strange (1932)&#39; b&#39;Storefront Hitchcock (1997)&#39;<br> b&#39;Celebration, The (Festen) (1998)&#39;]<br><strong>Recommendations for model (), user 1:</strong><br> [b&#39;Alien (1979)&#39; b&#39;Booty Call (1997)&#39;<br> b&#39;Day the Earth Stood Still, The (1951)&#39; b&#39;Angel and the Badman (1947)&#39;<br> b&#39;Sea Wolves, The (1980)&#39; b&#39;Celebration, The (Festen) (1998)&#39;<br> b&#39;Midnight Cowboy (1969)&#39; b&#39;Desperado (1995)&#39; b&#39;Tingler, The (1959)&#39;<br> b&#39;Blue Chips (1994)&#39;]</pre><p>This is just a personal thought, but as user 1 I think I would be pretty disappointed. The only children’s movie that appears in all of the recommendations is “Barney’s Great Adventure (1998)”, even though the user has a clear affinity for these kinds of movies. Plus, we can even see some inappropriate movies (user 1 is categorized as age K-12). Some more modeling improvement and placement of heuristics are needed before this system is production-ready.</p><p>Now, let’s view some accuracy results and see if we managed to beat the tutorial benchmark by including more features in the training process.</p><h3>Part IV — Beat the benchmark — Results</h3><p>We will train models with the following feature combinations:</p><ol><li>movie_title, user_id</li><li>movie_title, user_id, timestamp</li><li>movie_title, user_id, movie_title_text</li><li>movie_title, user_id, timestamp, movie_title_text <strong>(this setting was used in the official tutorial)</strong></li><li>movie_title, user_id, timestamp, movie_title_text, movie_genres</li><li>movie_title, user_id, timestamp, movie_title_text, movie_genres, movie_length</li></ol><p>And use the following layer sizes:</p><p>[64, 32]</p><p>For a total of 6 trained models. We will train each model for 300 epochs and use an embedding size of 32 for all embedding layers. I hypothesize that the additional features will enrich the item/query embeddings and help generate more accurate representations. The metric we will use to compare the models is top-100 accuracy (higher = better). This means, for each item/query pair in the validation set — was the affinity of the pair in the top-100 pairs generated for this query?</p><p>Shown in figure 4 (included a static image as well as an interactive graph because of visibility issues on mobile) is the validation top-100 accuracy (in 5 epoch intervals) for each model training.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*szA0EDkEUgEjgV07NE0glQ.png" /><figcaption>Fig. 4 (static): Model comparison results</figcaption></figure><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Freports%2F9ArQBZk%2Fmovielens-1m-tfrs%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Freports%2F9ArQBZk%2Fmovielens-1m-tfrs%2F&amp;image=https%3A%2F%2Fdatapane.com%2Fstatic%2Fpython_report_illustration.png&amp;key=d04bfffea46d4aeda930ec88cc64b87c&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/7d851165a03ff9f9ba06bc1dea27de1d/href">https://medium.com/media/7d851165a03ff9f9ba06bc1dea27de1d/href</a></iframe><p>We can see consistently that as we add more features, the validation accuracy increases. This is encouraging as it means our work had a positive effect! Note that the overall accuracy seems pretty low, this is due to this being a very difficult problem (need to guess the correct movie in the top-100 results out of around 6000 movies). The best approach in terms of validation accuracy was the one that used all of the proposed features, and indeed it beat the benchmark from the tutorial by about 0.015 after 300 epochs.</p><h3>Part V — Conclusions and thoughts ahead</h3><p>In this article we have seen:</p><ol><li>How to build a recommendation system using tensorflow recommenders.</li><li>How to add context features to both the items and the queries to achieve better performance.</li><li>How to perform research via plain scripting rather than using Jupyter notebooks.</li></ol><p>We can see that additional context, both for the queries and for the items, can boost our recommendation performance (even on relatively dense datasets such as MovieLens-1M). When building your own recommender systems try to think if you can add some more context to your embeddings rather than just using user and item identifiers :)</p><p>See you next time,</p><p>Elad</p><p><strong>References:</strong></p><p>[1]<em> Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. </em><a href="https://doi.org/10.1145/2827872"><em>https://doi.org/10.1145/2827872</em></a></p><p>[2] Tensorflow deep retrieval systems tutorial — <a href="https://www.tensorflow.org/recommenders/examples/deep_recommenders">https://www.tensorflow.org/recommenders/examples/deep_recommenders</a></p><p>[3] Cinemagoer python package — <a href="https://cinemagoer.github.io/">https://cinemagoer.github.io/</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4ca358cc886e" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/movielens-1m-deep-dive-part-ii-tensorflow-recommenders-4ca358cc886e">MovieLens-1M Deep Dive — Part II,  Tensorflow Recommenders</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
  
    <h2>MovieLens-1M Deep Dive — Part I</h2>
    <p><h3>MovieLens-1M Deep Dive — Part I</h3><h4>A hands-on recommendation systems tour using the popular benchmark dataset</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iDGcwsySEgnvyrm8PtQT3g.jpeg" /><figcaption>Photo from <a href="https://www.pexels.com/photo/food-snack-popcorn-movie-theater-33129/">pexels</a></figcaption></figure><p>Recommendations. We all consume them. Be it through our favorite movie streaming apps, online shopping, or even passively as a target of advertising campaigns. How are these recommendations created? How does a recommendation system utilize enormous datasets of internet transactions to generate high quality and personalized recommendations? I find these questions fascinating, hence I decided to embark on a learning journey and this post is intended to share my findings with you, dear readers :)</p><p>I find that there are sufficient theoretical blog posts on the basics of recommendation systems and decided that this will be a hands-on practical tour of one of the most popular recommendation-focused datasets available — <a href="https://grouplens.org/datasets/movielens/1m/">MovieLens-1M</a> [1] (used with permission). In this dataset, we are given ~1 million historical ratings of 2894 movies by 6040 unique users.</p><p>The structure of this post is as follows:</p><ul><li>Part I — Exploratory data analysis (EDA)</li><li>Part II — Data pre-processing (retrieving movie content information)</li><li>Part III — Setting up the recommendation problem and obtaining a baseline score</li><li>Part IV— Recommending movies with collaborative filtering</li><li>Part V — Recommending movies with content-based filtering</li><li>Part VI — Conclusions and thoughts ahead</li></ul><p>The notebook containing all of this project’s code and more is available here: <a href="https://colab.research.google.com/drive/132N2lGQTT1NnzAU6cg1IfwpaeFKK9QQe?usp=sharing">https://colab.research.google.com/drive/132N2lGQTT1NnzAU6cg1IfwpaeFKK9QQe?usp=sharing</a></p><p>This is the first of a two-part deep dive into MovieLens-1M. The next part, which will be released in the near future, will focus on more advanced algorithms and continue the never-ending EDA process. Happy reading!</p><h3>Part I — EDA</h3><p>First, download the MovieLens-1M dataset from <a href="https://grouplens.org/datasets/movielens/1m/">here</a>. Then, let’s make some necessary imports and get that out of the way for this project:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5457a2a7f9545da680f2f38e51cb53a7/href">https://medium.com/media/5457a2a7f9545da680f2f38e51cb53a7/href</a></iframe><p>The MovieLens-1M dataset consists of 3 files — users.dat, ratings.dat and movies.dat. Let’s explore each of these files and understand what we are dealing with. We will start with the movie&#39;s data.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/034ecd15ab58464f82b1340676d78404/href">https://medium.com/media/034ecd15ab58464f82b1340676d78404/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d706c36c2998588808611929068fb8b9/href">https://medium.com/media/d706c36c2998588808611929068fb8b9/href</a></iframe><p>Let’s plot the distribution of movies per genre. For this, we will have to turn the genres column into a list and “explode” it. Note that each movie may be of several genres, and in this case, it will be counted multiple times. We will also plot the distribution of movies per movie release year.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/92b251d5ea431508279a9690ca26d8d5/href">https://medium.com/media/92b251d5ea431508279a9690ca26d8d5/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2Fv7Jb297%2Fmovie-count-by-genre%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2Fv7Jb297%2Fmovie-count-by-genre%2F&amp;image=https%3A%2F%2Fdatapane.com%2Fstatic%2Fpython_report_illustration.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/edc01c5445a19c2eaf79fd9c94ad3364/href">https://medium.com/media/edc01c5445a19c2eaf79fd9c94ad3364/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f69c7659ed86bab810dc5f0ccdfc8b8e/href">https://medium.com/media/f69c7659ed86bab810dc5f0ccdfc8b8e/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FVkBgW8A%2Fmovie-count-by-year%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FVkBgW8A%2Fmovie-count-by-year%2F&amp;image=https%3A%2F%2Fdatapane.com%2Fstatic%2Fpython_report_illustration.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/bca43dcd567845564eff025272b7b527/href">https://medium.com/media/bca43dcd567845564eff025272b7b527/href</a></iframe><p>Let’s move on to the users’ data and inspect it. The “occupation” column is encoded with a number representing each occupation, but for EDA we are interested in the actual data. We will get it by extracting the relevant rows from the README file of the dataset and swapping values in the dataset accordingly.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/da88426646db26797097a3f7054e4341/href">https://medium.com/media/da88426646db26797097a3f7054e4341/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5412807928a4ba0e95315d913ea063e5/href">https://medium.com/media/5412807928a4ba0e95315d913ea063e5/href</a></iframe><p>Let’s view the ratings now.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/55e0770900b0f70de46ac6ea91c715f4/href">https://medium.com/media/55e0770900b0f70de46ac6ea91c715f4/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/e4b729dcd2ff8ace4773adcbf01b1c51/href">https://medium.com/media/e4b729dcd2ff8ace4773adcbf01b1c51/href</a></iframe><p>Let’s rank movie genres by their average rating. We will also plot how many times each genre was rated.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1d557661e0921e862ce8abfdbff4f409/href">https://medium.com/media/1d557661e0921e862ce8abfdbff4f409/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FaAMbQW3%2Fratings-by-genre%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FaAMbQW3%2Fratings-by-genre%2F&amp;image=https%3A%2F%2Fstorage.googleapis.com%2Fdatapane-files-prod%2Fdp%2Fthumbnails%2F87716df9-6c9a-4b6a-9f71-f412fa6cb226.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/0e6fa4828d29ed920755b98b700b8a73/href">https://medium.com/media/0e6fa4828d29ed920755b98b700b8a73/href</a></iframe><p>Now let’s combine all three tables (movies, users, and ratings) and see if there are any differences between male and female ratings per movie genre. Notice that here too we will normalize by the total amount of males/females to get a better answer to our question.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8ccf0f5ad4ed01c8528a23e6f5ec60e3/href">https://medium.com/media/8ccf0f5ad4ed01c8528a23e6f5ec60e3/href</a></iframe><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2Fj3LYl47%2Fratings-by-gender-and-genre%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2Fj3LYl47%2Fratings-by-gender-and-genre%2F&amp;image=https%3A%2F%2Fdatapane.com%2Fstatic%2Fpython_report_illustration.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/4e62b19af3561df82bde93e564f4bf67/href">https://medium.com/media/4e62b19af3561df82bde93e564f4bf67/href</a></iframe><p>Again, we see some typical male/female differences in this dataset (men gave higher ratings to actions movies than women and vice versa for romance movies). This is a good sanity check for the data as the plots make sense.</p><h3>Part II — Data pre-processing (retrieving movie content information)</h3><p>In this part we will retrieve an important data source that will help us recommend movies later, and this is the movie plot summaries. We will obtain them from Wikipedia, using the <a href="https://pypi.org/project/wikipedia/">Wikipedia python package</a>.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a92dc02cd7d40472cf0ed844f7b20b13/href">https://medium.com/media/a92dc02cd7d40472cf0ed844f7b20b13/href</a></iframe><p>There are 615 NaN movie plots</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f7c1376e79484c49a3559ff35e2d7eeb/href">https://medium.com/media/f7c1376e79484c49a3559ff35e2d7eeb/href</a></iframe><p>We see that we were not able to obtain movie plots for 615 movies. This might be because their Wikipedia page doesn’t have a “Plot” category or for some other reason. Since web crawling isn’t the main point here, we’ll just drop these movies and all ratings associated with them.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ccf0172f91a763d0f6cd34e1f53b47b5/href">https://medium.com/media/ccf0172f91a763d0f6cd34e1f53b47b5/href</a></iframe><h3>Part III — Setting up the recommendation problem and obtaining a baseline score</h3><p>We will choose a user that will be acase study for which we will make actual predictions and see if the make sense to us. Most conveniently, we will choose the user with ID #1. Shown below in the user and the movies that she rated. We can see that she is a female K-12 student (age of 1 is an error in the data but I don’t think it’s very important anyhow). The top-20 movies she likes are mainly classical children’s hits with some anomalies here and there (Schindler’s List, One Flew Over the Cuckoo’s Nest).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/555/1*EtuZbHe2-d2yY56u0YA9mQ.png" /></figure><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6084a01144a6898623ec6b4ac68f2b4d/href">https://medium.com/media/6084a01144a6898623ec6b4ac68f2b4d/href</a></iframe><p>We will also shine our spotlight on the movie Toy Story (1995), and for each prediction method we will check which are the movies that are closest to this movie in the embedding space.</p><p>Finally, let’s start recommending stuff! We will create two dataset splits:</p><p>1. Standard train/test split. This will be used for the rating prediction regression task</p><p>2. Leave-one-out cross-validation split. This will be used for hit-rate prediction (which is generally regarded as a more relevant metric in the field of recommender systems). The split is performed by taking one movie per user out of the dataset and using this as a train set. All the other user’s movies will be on the trainset. Then, the hit rate is the percentage of times the movie we took out was in the top-K ratings for that user.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/42a4ce204ab4396b247f048ceda63abc/href">https://medium.com/media/42a4ce204ab4396b247f048ceda63abc/href</a></iframe><p>For each method we will calculate:</p><ul><li>RMSE on test set</li><li>hit-rate on leave-one-out cross validation set</li><li>Top predictions for user #1 (case study)</li><li>Top-10 most similar movies to Toy Story (1995)</li></ul><p>Now, let’s define some auxiliary functions to help us evaluate our algorithms (mainly — the hit rate). We will inspect two measures for each algorithm — the RMSE and hit rate. Some of the code here is taken from the fabulous course on recommender systems by <a href="https://sundog-education.com/recsys/">Sundog Education</a>. I will also implement a simple tweak to the original KNN and SVD algorithms because calculating the hit rate takes a long time and I want to at least be able to measure the progress in real-time.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a14edc9471d0f4e4578a73367f831113/href">https://medium.com/media/a14edc9471d0f4e4578a73367f831113/href</a></iframe><p>Let’s obtain results for a completely random recommender system, so we will be able to evaluate if our algorithms are better or worse than random (after all, we want to make things better not worse…)</p><ul><li>Note — hit rate takes a lot of time to compute</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a7c0974a38b4f0be9003907647337756/href">https://medium.com/media/a7c0974a38b4f0be9003907647337756/href</a></iframe><pre>RMSE: 1.4960<br>HitRate: 0.02185430463576159</pre><h3>Part IV — Recommending movies with collaborative filtering</h3><p>We will use the classic vanilla algorithms, out-of-the-box from the Surprise package (with some minor tweaks to show progress in KNN). I won’t go into the algorithmic details here because I think there are many great tutorials out there for learning these things (just google recommender systems SVD/KNN and you&#39;re good to go). I will check these algorithms:</p><ol><li>SVD</li><li>User-based KNN</li><li>Item-based KNN</li></ol><p>We will start with SVD:</p><pre>RMSE: 0.8809<br>HitRate: 0.03923841059602649</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/762/1*QA8122cc7PwbcuDZe8GYpw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/658/1*sK5FCZe54WvUuLryVdEP5g.png" /></figure><p>Nice! That’s a major improvement over the random score. But when we look at the predictions for user #1 they still feel a bit “off”. The only children’s movie is Babe (1995) and I personally would expect more of these to appear in a good recommendation for this user.</p><p>Now onto KNN. We will start with user-based KNN. This means that predictions for movie <em>m</em> and user <em>u</em> will be based upon the tastes of users that are similar to <em>u</em>.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8e42c76833f5faa9d4b4994d0e7250d2/href">https://medium.com/media/8e42c76833f5faa9d4b4994d0e7250d2/href</a></iframe><pre>RMSE: 0.9595<br>HitRate: 0.002152317880794702</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/656/1*WLGhvvYSlI0JJzp_EmDopg.png" /></figure><p>The RMSE is better than random but the hit rate is worse. We can also see that the top predictions for user_1 seem even further away than those that were given by the SVD algorithm. In user-based KNN there is no notion of similarity between items, so we won’t calculate the most similar movies to Toy Story (1995) for this algorithm.</p><p>Let us now check the performance of item-based KNN, which recommends items to users by the similarity of the items they already rated to other items they have not yet rated. Code-wise only a slight change to the class instantiation is required.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/632f7953b5ab1b624276cd03beec790b/href">https://medium.com/media/632f7953b5ab1b624276cd03beec790b/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/869/1*tU1iUEHRFmQPwoF6t_rv6w.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/713/1*DyzxsHX5EVQQ4GobjbOnDA.png" /></figure><pre>RMSE: 0.9851<br>HitRate: 0.002152317880794702</pre><p>The RMSE is a bit worse than the user-based KNN and the hit rate reaches almost zero. The most similar movies to Toy Story (1995) seem reasonable but the predictions for user_1 are once again unsatisfying. So far the KNN algorithms have failed to outperform the SVD, which really is regarded as a much better algorithm for collaborative filtering.</p><p>Up until now, we haven’t taken into account the features of the movies themselves. Maybe if I know something about the details of the movies it will help me make better recommendations? Let’s check it out. This is called content-based filtering.</p><h3>Part V — Recommending movies with content-based filtering</h3><p>For the content-based filtering we will use KNN-based algorithms in three approaches (two of them item-based and one user-based):</p><p>1. <strong>Movie plots (item-based)</strong>: Create a vector representation of all of the movies based on the plot descriptions. We will do this by first stemming all of the words in the plot description and then applying TF-IDF to vectorize each document. The similarity matrices we will generate will be based on:</p><p>a. Using the complete TF-IDF matrix</p><p>b. Using the TF-IDF matrix after feature selection</p><p>c. Using the TF-IDF matrix after feature selection and after removing peoples’ names</p><p>2. <strong>Movie genres (item-based)</strong>:<strong> </strong>We will use the movie genres as the only source for recommendations and see how that goes.</p><p>3. <strong>User age+gender (user-based)</strong>: We will use user data as features for our KNN predictor.</p><p>We will create a class that inherits from Surprise’s KNNBasic. Its functionality will be the same as item-based KNN for collaborative filtering, with the only difference being that we will supply a pre-calculated similarity matrix to the fit function, instead of having it calculated from the rating data.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/fccd73cfeca3f7d5b3449d4395fd670c/href">https://medium.com/media/fccd73cfeca3f7d5b3449d4395fd670c/href</a></iframe><p><strong>Part V.I — Content-based filtering using movie plots</strong></p><p>Now, let’s create the TF-IDF similarity matrix</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ebba7c4991de58e2f1899cc4f61131f4/href">https://medium.com/media/ebba7c4991de58e2f1899cc4f61131f4/href</a></iframe><p>Now, get the results for approach #1 (using the complete TF-IDF matrix). Note that we need a different cosine similarity matrix for the regular trainset and the leave-one-out trainset which is used to calculate hit-rate because they contain different inner ids for each movie.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7dd1796b0061a858225f1ab960193852/href">https://medium.com/media/7dd1796b0061a858225f1ab960193852/href</a></iframe><pre>RMSE: 1.0268<br>HitRate: 0.003973509933774834</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/726/1*jpIh-sFcE_RNjs0ax8Pqtg.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/658/1*XklBowYBxwV3VbCWwtwRdQ.png" /></figure><p>Worse than collaborative filtering, without a doubt. We also see that the similarity matrix doesn’t really generate meaningful relationships between movies (the only children’s movie which is similar to Toy Story is Toy Story 2). Let’s see if reducing the number of features helps.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f39f2e89ce2acaf7f0173a22fd2e804d/href">https://medium.com/media/f39f2e89ce2acaf7f0173a22fd2e804d/href</a></iframe><pre>Number of selected features:  784</pre><pre>List of selected features:   [&#39;abbi&#39;, &#39;abel&#39;, &#39;ace&#39;, &#39;adam&#39;, &#39;adel&#39;, &#39;adrian&#39;, &#39;adrien&#39;, &#39;affair&#39;, &#39;agent&#39;, &#39;agn&#39;, &#39;al&#39;, &#39;aladdin&#39;, &#39;alan&#39;, &#39;albert&#39;, &#39;alex&#39;, &#39;alfr&#39;, &#39;ali&#39;, &#39;alic&#39;, &#39;alicia&#39;, &#39;alien&#39;, &#39;allen&#39;, &#39;alli&#39;, &#39;alvin&#39;, &#39;alyssa&#39;, &#39;amanda&#39;, &#39;amelia&#39;, &#39;american&#39;, &#39;ami&#39;, &#39;amo&#39;, &#39;andi&#39;, &#39;andrea&#39;, &#39;andrew&#39;, &#39;angel&#39;, &#39;angela&#39;, &#39;angelo&#39;, &#39;angus&#39;, &#39;ann&#39;, &#39;anna&#39;, &#39;anni&#39;, &#39;antoin&#39;, &#39;anton&#39;, &#39;antonio&#39;, &#39;ape&#39;, &#39;archer&#39;, &#39;archi&#39;, &#39;ariel&#39;, &#39;arjun&#39;, &#39;armstrong&#39;, &#39;arni&#39;, &#39;arroway&#39;, &#39;art&#39;, &#39;arthur&#39;, &#39;arturo&#39;, &#39;ash&#39;, &#39;audrey&#39;, &#39;aurora&#39;, &#39;austin&#39;, &#39;axel&#39;, &#39;babe&#39;, &#39;babi&#39;, &#39;balto&#39;, &#39;bambi&#39;, &#39;band&#39;, &#39;bank&#39;, &#39;barbara&#39;, &#39;barn&#39;, &#39;barney&#39;, &#39;bastian&#39;, &#39;bate&#39;, &#39;bateman&#39;, &#39;batman&#39;, &#39;beach&#39;, &#39;beal&#39;, &#39;bean&#39;, &#39;beatric&#39;, &#39;beckett&#39;, &#39;becki&#39;, &#39;beldar&#39;, &#39;bella&#39;, &#39;ben&#39;, &#39;bendrix&#39;...</pre><pre>RMSE: 1.0314<br>HitRate: 0.003642384105960265</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*BWj1x-b5fQQFrlqJcHrO5g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/708/1*oBGvA1nu2OzbNWc5sKArhw.png" /></figure><p>Didn’t really help…We see that a large portion of the informative features are names. We don’t want that because names don’t really say anything about the movie content. Let’s remove the names.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cc12754ada45305fa33f5857124b14bb/href">https://medium.com/media/cc12754ada45305fa33f5857124b14bb/href</a></iframe><pre>RMSE: 1.0284<br>HitRate: 0.0041390728476821195</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/891/1*NGYrdRAUAZwgIARswe4d-g.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/676/1*VZy1UZht1cB3pXtI4G1r-w.png" /></figure><p>OK, this is somewhat more reasonable than the previous two approaches. First of all the hit-rate is higher (though still way less than the collaborative filtering methods), and second the similar movies to Toy Story(1995) are actually similar. Third, the top predictions for user_1 even seem passable. Let us now try the movie-poster approach and see how that goes.</p><p><strong>Part V.II— Content-based filtering using movie genres</strong></p><p>In this part I will tune down the algorithmic complexity and create a KNN recommender based only on the movie genres. This means that if a person rated children’s movies very high, we should expect the algorithm to recommend children’s movies. We will create all possible combinations of genres using pythons itertools package and then apply our old friend TF-IDF to generate a feature matrix from this data.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ba305925548dfc4bb3c303e6e623a1a2/href">https://medium.com/media/ba305925548dfc4bb3c303e6e623a1a2/href</a></iframe><pre>RMSE: 1.0138<br>HitRate: 0.0031456953642384107</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/885/1*2UzsMJUEYam1xfC1XEKbkw.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/535/1*sksGjMCf2SagW71XPB6Jig.png" /></figure><p>Unsurprisingly the most similar movies to Toy Story (1995) are those that have identical or almost identical genres. Other than that the results are quite confusing, why did user_1 get recommendations only for war movies? Let’s remind ourselves of the table in part III of this article (which shows all of the ratings of user_1), we see that there are two movies with genre “war” in user_1’s rating list and both of them got a score of 5. The other genres that user_1 has rated appear more frequently and thus if we average user_1’s scores by genre — the “war” genre will receive a perfect score while the other genres won’t.</p><p>Because the item content-based KNN recommender predicts new items for user_1 by their similarity to other items that have been scored by user_1 it will give a perfect score for each war movie it will see in the context of user_1! The KNN recommender ignores scores belonging to 0 similarity items, it will only take into account the war movie scores when generating the recommendations for war movies and thus always predict perfect scores.</p><p><strong>Part V.III — Content-based filtering using user User age+gender</strong></p><p>Let us start by creating a feature vector from the users’ age and gender. We will assign 0 to ‘Male’ and 1 to ‘Female’ and we will take the age column as-is. Finally we will normalize the columns so as not to be affected by the difference in column magnitudes. Using these values we will create a cosine-similarity matrix as we did for the movies, but this time we will calculate a similarity score between each user. This similarity score will be an input to our user-based KNN recommender.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/eb013182b1375287f9da881a55abac39/href">https://medium.com/media/eb013182b1375287f9da881a55abac39/href</a></iframe><pre>RMSE: 0.9784<br>HitRate: 0.00347682119205298</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/771/1*C3BAeCkUQmfqxy6j5Yn1Ug.png" /></figure><p>We see that the RMSE is a better than for the item-based methods, similarly to the results we got in part IV (collaborative filtering), where the user-based recommendations outperformed item-based ones. but the hit rate is a bit worse than some of the item-based methods we tried. It seems that each method has its strengths and weaknesses and a good approach will probably be to somehow combine them all, and this will be the focus of the next part.</p><h3>Part VI — Conclusions and thoughts ahead</h3><p>In this post, a lot was covered.</p><p>First, we inspected the MovieLens-1M dataset and got some pretty interesting insights from the graphs we saw, such as which movie genres tend to score higher than others in average. We then used some vanilla recommender systems algorithms from the Surprise python package, and got some pretty good results with the SVD algorithm.</p><p>After that, we tried our luck with content-based filtering, but unfortunately, this turned out to be a futile attempt. Despite that, in the research process we unveiled an inherent weakness in KNN recommender systems (or content-based filtering in general), which is that the recommendations for a specific user are based solely on the items that he/she has interacted with in the past, and if there was bias in this user’s interactions it will show up in the KNN recommendations. This is why our spotlight user (user_1) got top scores for war movies when we used genres as the basis of our recommendations, even though we as humans wouldn’t necessarily agree. A more probable recommendation for this user would be children’s/musicals and the like.</p><p>Additionally, we saw that when dealing with natural text, such as movie plots, it is worthwhile to understand which features are most affecting the recommendations. When we removed person names which were ranked very high in the TF-IDF algorithm, we suddenly got better results and this was clearly visible when the movies that were most similar to Toy Story (1995) suddenly made sense.</p><p>In the next part, I will combine content-based filtering and collaborative filtering and thus have the best of both worlds. I already have my eyes on the <a href="https://www.tensorflow.org/recommenders">TensorFlow-recommenders</a> package and I cant’ wait to give it a go.</p><p>Until next time!</p><p>Elad</p><p><strong>References:</strong></p><p>[1]<em> Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1–19:19. </em><a href="https://doi.org/10.1145/2827872"><em>https://doi.org/10.1145/2827872</em></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8acfeda1ad4" width="1" height="1" alt=""><hr><p><a href="https://towardsdatascience.com/movielens-1m-deep-dive-part-i-8acfeda1ad4">MovieLens-1M Deep Dive — Part I</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
  
    <h2>Create a simple GUI image processor with PyQt6 and OpenCV</h2>
    <p><p>Hi all,</p><p>This is a super-short weekend project. We are going to create a simple GUI application in python which highlights specific colors in an image and reports which percentage of the image was highlighted. Let’s show an example of the GUI:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GPVq4azfdApFfTtP40X5EA.png" /><figcaption>Given a source image of an incredible red Volvo, some input regarding the color we want to trace, and the tracing threshold, our program outputs the traced image and reports the percentage of the image which was traced.</figcaption></figure><p>Let’s start our project with some necessary imports:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/46f4b877c07fd23d9e28296a20f56746/href">https://medium.com/media/46f4b877c07fd23d9e28296a20f56746/href</a></iframe><p>Let’s create a MainWindow class which will be the one and only window in our application. We subclass the PyQt class QMainWindow and customize it to suit our needs. The main layout of this application will be a QVBoxLayout which stacks widgets vertically. Inside this layout, we will nest some QHBoxLayouts, which stack widgets horizontally. We also define additional variables which will be of use later in the code.</p><p>Let us continue the code for this class. We define the “select image” and “process buttons” and link their clicked event to some functions we will define later. We also set the RGB selectors, where each of them is a QSpinBox. We must define the maximum and minimum values so a user won’t enter illegal values (only numbers between 0 and 255 are valid RGB values).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/b71c69856ecc3bf2348d9090ad4d8c27/href">https://medium.com/media/b71c69856ecc3bf2348d9090ad4d8c27/href</a></iframe><p>We continue with the MainWindow class. We add the previously created widgets to the top_bar_layout. Then we start creating the widgets in the middle bar — image_bar_layout. This layout contains the source and processed images, including their labels. We also limit the image sizes by using the setMaximumSize methods, so our screen won’t explode with high-res images.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8cff4a21d7d9b1a8084ed8c32a43405a/href">https://medium.com/media/8cff4a21d7d9b1a8084ed8c32a43405a/href</a></iframe><p>The following code ends the __init__() function of the MainWindow class. We define the bottom_bar_layout which will contain the “Save as file” button to save the processed image. It will also contain a label that will report which percentage of the image has been traced. We will end with the setCentralWidget method that will tell the MainWindow which widget to use. We set a dummy widget called widget whose whole purpose is to contain main_layout, which contains the “meat” of this window.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/45ec4421b685432734f8f3a1b63cceba/href">https://medium.com/media/45ec4421b685432734f8f3a1b63cceba/href</a></iframe><p>The following code contains the remaining methods in the MainWindow class. choose_source_image is used to select an image from the local filesystem. Notice that we resize the image. This is support the processing of larger files by the program (Qt limits image sizes to 128 megabytes).</p><p>process_image does the actual tracing by color. It later resizes the result image and sets the previously defined self.result_image to show the result.</p><p>save_as_file simly utilizes the imwrite function of opencv to save the result in the local file system, given a filename obtained from a filesystem dialog.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2a8866a9cf1e2a08dab616102f76bcc4/href">https://medium.com/media/2a8866a9cf1e2a08dab616102f76bcc4/href</a></iframe><p>The following are helper functions which 1) resize the image using opencv, 2) create a Qt pixmap from an opencv image, and 3) apply the tracing process, which is the whole purpose of our program.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7f2600ab964b94192d2989c7fa48ab1c/href">https://medium.com/media/7f2600ab964b94192d2989c7fa48ab1c/href</a></iframe><p>Finally, here is the whole code from start to finish:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6267b87462c983d5be9b6551d7a1749b/href">https://medium.com/media/6267b87462c983d5be9b6551d7a1749b/href</a></iframe><p>As a bonus, to create a python executable from this, all you need to do is pip install pyinstaller and run pyinstaller image_tracer.py from the directory containing the code!</p><p>Thanks all, see you in the next post :)</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1821e1463691" width="1" height="1" alt=""><hr><p><a href="https://towardsdev.com/create-a-simple-gui-image-processor-with-pyqt6-and-opencv-1821e1463691">Create a simple GUI image processor with PyQt6 and OpenCV</a> was originally published in <a href="https://towardsdev.com">Towards Dev</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p></p>
  
    <h2>Foresee the Catastrophe — predicting turbo engine faults using machine learning</h2>
    <p><h3>Foresee the Catastrophe — predicting turbo engine faults using machine learning</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Qj84rm9NE-5tryAMBypyUQ.jpeg" /></figure><p>Hello Medium. I have decided to pick up on the super-relevant subject of predictive maintenance and start with the industry gold standard — the NASA Turbofan Engine Degradation Simulation Data Set (<a href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan">https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan</a>).</p><p>My journey was highly influenced by <a href="https://medium.com/u/769bd197e7cf">Koen Peters</a>’s <a href="https://towardsdatascience.com/predictive-maintenance-of-turbofan-engines-ec54a083127">awesome series</a> regarding the analysis of this dataset and his solutions to the problem.</p><p>The dataset is a set of four different datasets: FD001, FD002, FD003, FD004 which are different variations of the same problem. This post will focus on the first dataset: FD001 .</p><p>Each of these datasets consists of numerous multivariate time series of simulated sensor measurements from a fleet of engines. The dataset was used as part of the Prognostics and Health Management (PHM) data competition at PHM’08. The contestants in this challenge were not given any background into the sensor data and were asked to perform pure data-driven predictions, thus we will act accordingly.</p><p>Each time series of sensor measurements starts with a different degree of engine wear and ends at system failure. In other words, we have data from <em>N </em>different scenarios of engine system failures, with <em>N</em> being the number of time individual time series. The test set in each dataset consists of time series with sensor measurements until a certain point in time before system failure which is referred to as remaining useful lifetime (RUL). The goal is to predict the RUL for each time series in the test set as accurately as possible.</p><p>The layout of this post is as follows:</p><ul><li><strong>Part I — Exploratory data analysis (EDA): </strong>In this part, we will perform some basic plotting of the data and understand things such as which sensors contain relevant information and which do not, and we will try to understand the overall behavior of the data concerning the predicted variable.</li><li><strong>Part II — Baseline model: </strong>Here we perform some basic preprocessing and implement a very naive solution to predict the RUL for each time series in the test set. The results for this part will be our baseline score to beat when we try other, more advanced, methods.</li><li><strong>Part III — Feature engineering:</strong> Here we will try two methods of feature engineering. One automatic, using the python package tsfresh , and the other manual, using common sense :)</li><li><strong>Part IV — LSTM</strong>: In this part, we will use an LSTM neural network to process the raw data and predict the RUL for each time series in the test set.</li></ul><h3><strong>Part I — EDA</strong></h3><p>We will start with the necessary imports. These are for the whole project.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/034389fdc7249ccdecba09b3e86c8528/href">https://medium.com/media/034389fdc7249ccdecba09b3e86c8528/href</a></iframe><p>Now we will start by loading the data and viewing a description of each column:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/83c66c4fba9449b246b716f22612527a/href">https://medium.com/media/83c66c4fba9449b246b716f22612527a/href</a></iframe><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/380f2750e8202653c69400a6cea72122/href">https://medium.com/media/380f2750e8202653c69400a6cea72122/href</a></iframe><p>And view the first five rows as well:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/6d9bede767bc6a808dd68ede39db3612/href">https://medium.com/media/6d9bede767bc6a808dd68ede39db3612/href</a></iframe><p>We immediately suspect sensors 3, 4, 8, 9, 10, 11, 13, 19, 21, 22 to be non-informative because their standard deviation is very low in relation to their mean value. Sensors 25, 26 contain only NaNs, so we can just drop them.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/5e71ccf173320f1d201cea9721498959/href">https://medium.com/media/5e71ccf173320f1d201cea9721498959/href</a></iframe><p>Usually, having no domain knowledge hinders our ability to extract useful information from the data. Despite this, we will use this to our advantage. Because we don’t know anything about the individual sensors, we can start with normalizing all the data to be on the same scale, this will make our analyses much easier.<br>We will fit a MinMaxScalerto each sensor column, and save these scalers for later use (we will need to perform the same transformation on the test data). After that, we will plot the first unit’s measurements of each signal to verify our assumptions regarding which signals don’t contain relevant information.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/ea9c35f3dc10529453a08c7a5c26dd39/href">https://medium.com/media/ea9c35f3dc10529453a08c7a5c26dd39/href</a></iframe><p>The interactive plot below, of the first sensor recording in the dataset, allows us to visually divide the signals into four groups:</p><ul><li>irrelevant 3, 4, 8, 9, 13, 19, 21, 22</li><li>upward trend: 5, 6, 7, 11, 14, 16, 18, 20</li><li>downward trend: 10, 12, 15, 17, 23, 24</li><li>no trend: 1, 2</li></ul><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FBAmpn1k%2Fnasa-fd001%2Fembed%2F&amp;display_name=Datapane&amp;url=https%3A%2F%2Fdatapane.com%2Fu%2Felad%2Freports%2FBAmpn1k%2Fnasa-fd001%2F&amp;image=https%3A%2F%2Fstorage.googleapis.com%2Fdatapane-files-prod%2Fdp%2Fthumbnails%2F78fb5478-c35d-4289-bb99-4c70de367a13.png&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=datapane" width="960" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/333685eda22791597e542fedd9a15000/href">https://medium.com/media/333685eda22791597e542fedd9a15000/href</a></iframe><h3>Part II— Baseline Model</h3><p>We will now drop the irrelevant columns, and continue to add labels to the data — the RUL. Our first labeling method will be naive and straightforward. We know that each unit in the training set is run until RUL = 0. Thus, we will assume that the degradation is linear and label each time step in the training set accordingly.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d3d419a0ccc454238123cc06348afc79/href">https://medium.com/media/d3d419a0ccc454238123cc06348afc79/href</a></iframe><p>Great, now every row has a label. We will now perform regression where each row is a data sample, with x = row sensors and y = RUL. As in Koen Peters&#39; post (<a href="https://towardsdatascience.com/predictive-maintenance-of-turbofan-engines-ec54a083127">https://towardsdatascience.com/predictive-maintenance-of-turbofan-engines-ec54a083127</a>), we clipped the RUL values at a maximum value of 125 to match the engine degradation trend better.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cec7708f9e84d55c1f68d8b100446cb3/href">https://medium.com/media/cec7708f9e84d55c1f68d8b100446cb3/href</a></iframe><p>Alright, we have our first model. We will use a vanilla XGBoost model throughout the post because of it is generally considered a very good performing model out-of-the-box. Now, let’s load the test set (and not forget to apply the scalers to the test set as well). Below we can see our baseline performance on the train and test sets.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d2dbdff45acabb88902630ae9c7b91bd/href">https://medium.com/media/d2dbdff45acabb88902630ae9c7b91bd/href</a></iframe><p>Output:</p><pre>RMSE on train set: 17.679585838619715<br>RMSE on test set: 17.35303931484297</pre><p>Alright, so 17.353 is our test RMSE to beat in the following sections.</p><h3>Part III — <strong>Feature engineering</strong></h3><h4>Part IIIa — Windowing the data + a naive regression approach</h4><p>Instead of taking each feature instance to be one timestep of sensor measurements, let’s try using a window of 20 timesteps. The rational behind this is that maybe data from the near past will help determine the RUL of the current timestep.<br>This operation of course will leave us with 3D data (n_samples, timesteps, n_sensors), which we will have to featurize or flatten in order to keep using traditional ML methods.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2fd82d2fd22461d1e830018eca1f7572/href">https://medium.com/media/2fd82d2fd22461d1e830018eca1f7572/href</a></iframe><p>Our first attempt at this windowed approach will be to flatten the dimensions of each window and simply feed all of it to XGBoost. This will of course hide the time-related information from the model.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0bea32570e4d6734a8c8351ec41801a4/href">https://medium.com/media/0bea32570e4d6734a8c8351ec41801a4/href</a></iframe><p>Output:</p><pre>RMSE on train set: 15.908527707436281<br>RMSE on test set: 16.429387222606096</pre><p>Surprisingly, this approach improves our test RMSE. The XGBoost model was able to make use of the previous 19 samples for each sensor to improve its RUL prediction ability, despite the unstructured nature of the input data (one long array containing measurements from all sensors).</p><h4>Part IIIb — Automatic feature generation</h4><p>Let us now try to generate features from each mini-time series of 20 time steps. We will use the python package tsfresh which generates features automatically from a set of fixed operations.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0ce281ee3298fc62a5e11a0fc8bc2c8a/href">https://medium.com/media/0ce281ee3298fc62a5e11a0fc8bc2c8a/href</a></iframe><p>Output:</p><pre>RMSE on train set: 14.486881328112556<br>RMSE on test set: 23.113512123440437</pre><p>For computational efficiency we used only the features that are not computationally heavy to calculate. For each sensor in each window the following were calculated as features: root_mean_square, mean, median, sum, maximum, minimum, variance and standard_deviation. This resulted in a dataset containing 118 features.</p><p>We can see that the results are extremely overfitted for this method and it proves to be quite unsuccessful compared to the naive windowing approach.</p><h4>Part IIIc— Manual feature generation</h4><p>In this part, we will apply several operations to the measurements of each sensor, in each window. We will avoid creating features that combine sensors because we lack the domain knowledge (for example, the meaning of the sensors) required to generate these kinds of combinations. This approach is quite similar to the automatic feature generation, except the np.difffunction, which computes the difference between each two executive measurements, that was used here and isn’t used in the automatic version.<br>The operations for each sensor will be as follows:</p><ul><li>average value</li><li>average diff value</li><li>standard deviation</li><li>maximum value</li><li>minimum value</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/86ecbcc1007c6b177f5e64e0af1b9bdd/href">https://medium.com/media/86ecbcc1007c6b177f5e64e0af1b9bdd/href</a></iframe><p>Output:</p><pre>RMSE on train set: 14.58791927615462<br>RMSE on test set: 18.131375854948576</pre><p>This approach is disappointing as well, with worse test RMSE than the naive windowing approach. Maybe we will have better luck with the LSTM model :)</p><h3>Part III — LSTM</h3><p>In this section we will use a simple LSTM network to process the data windows ans see if we gain any improvement from our baseline model (which the feature approach failed to deliver). We will use Pytorch lightning (the equivalent of Keras for Tensorflow) to streamline our model creation process.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3378a7dff2e43127c1263106633e436d/href">https://medium.com/media/3378a7dff2e43127c1263106633e436d/href</a></iframe><p>Now let’s create the LSTM model:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/55a0c99c80a5a649ab08f53bf7b8e2fe/href">https://medium.com/media/55a0c99c80a5a649ab08f53bf7b8e2fe/href</a></iframe><p>Now, create the trainer and fit the model with the training data:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/40a0f00074d147ec56515e954a0ab666/href">https://medium.com/media/40a0f00074d147ec56515e954a0ab666/href</a></iframe><p>Finally, get the train and test RMSE for the LSTM approach:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/10369d80c438204e6540b8383f5edd48/href">https://medium.com/media/10369d80c438204e6540b8383f5edd48/href</a></iframe><p>Output:</p><pre>RMSE on train set: 14.761564144403062<br>RMSE on test set: 16.065719776970667</pre><p>We can see that the LSTM approach yielded the best test RMSE for this dataset, and this concludes this article. For future work it is worth investigating the feature generation approach and the reasons for it’s failure. (maybe some domain knowledge will be of use). Moreover, it may be beneficial to experiment with other deep learning architectures and not just LSTM (for example CNNs, transformers).</p><p>Full notebook link: <a href="https://colab.research.google.com/drive/12O670WevZRLm4AvQ_JtyU5cHN5PMnB1C?usp=sharing">https://colab.research.google.com/drive/12O670WevZRLm4AvQ_JtyU5cHN5PMnB1C?usp=sharing</a></p><p>Hope you enjoyed reading!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cb8c0344474e" width="1" height="1" alt=""></p>
  
<section class="special">
  <ul class="actions">
  <li><a href="" class="button ">Read More</a></li>
  </ul>
</section>

<h2> Latest Blog Posts </h2>


      

      

    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="/feed.xml" title="RSS">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">RSS</span>
    </a>
  </li><li class="list-inline-item">
    <a href="mailto:someone@example.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://www.facebook.com/deanattali" title="Facebook">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-facebook fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Facebook</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/daattali" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://twitter.com/daattali" title="Twitter">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Twitter</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://www.youtube.com/c/daattali" title="YouTube">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-youtube fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">YouTube</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://patreon.com/DeanAttali" title="Patreon">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-patreon fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Patreon</span>
    </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Some Person
        &nbsp;&bull;&nbsp;
      
      2022

      
        &nbsp;&bull;&nbsp;
        <span class="author-site">
          <a href="">MyWebsite.com</a>
        </span>
      

      

      

      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/assets/js/beautifuljekyll.js"></script>
    
  









</body>
</html>
