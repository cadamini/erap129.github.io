---
layout: post
title: SoupNAS - Neural architecture search for stochastic neural networks
---

Using neural architecture search we can obtain high quality neural network architecture for various purposes. In this project I focus on the creation of stochastic neural networks, which are highly effective for reducing overfitting and can be trained in a distributed fashion. 

![_config.yml]({{ site.baseurl }}/images/bowl-of-soup-png-3-transparent.png)  

The goal in this project is to create neural networks which operate stochasitcally, at the layer level. This means that for a given neural network, each run may result in a different set of layers being activated. This can be thought of as a "soup" of tensors and layers, where different tensors undergo different transformations in a semi-random fashion until an output is generated.  
  
To ease the testing process, for all experiments I used a predefined CNN architecture template, consisting of three blocks, each containing 10 layers. Between each block is a reduction block, which includes a convolutional layer and max-pooling layer. The reduction block reduces the input's size by half, while doubling the number of channels (creating a smaller but "fatter" activation map). Inside each block the activation map's size stays the same and this allows the activation of random layers at a random order to be valid. In the initial experiments I used the Fashion-MNIST dataset (grayscale images of 28 x 28 pixels, of different clothing categories).  
  
![The block architecture]({{ site.baseurl }}/images/SoupNAS_blocks.png)
*The 3-block architecture template* 

