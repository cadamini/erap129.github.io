---
layout: post
title: SoupNAS - Neural architecture search for stochastic neural networks
---

Using neural architecture search we can obtain high quality neural network architecture for various purposes. In this project I focus on the creation of stochastic neural networks, which are highly effective for reducing overfitting and can be trained in a distributed fashion. 

![_config.yml]({{ site.baseurl }}/images/bowl-of-soup-png-3-transparent.png)  

The goal in this project is to create neural networks which operate stochasitcally, at the layer level. This means that for a given neural network, each run may result in a different set of layers being activated. This can be thought of as a "soup" of tensors and layers, where different tensors undergo different transformations in a semi-random fashion until an output is generated.  
  
To ease the testing process, for all experiments I used a predefined CNN architecture template, consisting of three blocks, each containing 10 layers. Between each block is a reduction block, which includes a convolutional layer and max-pooling layer. The reduction block reduces the input's size by half, while doubling the number of channels (creating a smaller but "fatter" activation map). Inside each block the activation map's size stays the same and this allows the activation of random layers at a random order to be valid. In the initial experiments I used the Fashion-MNIST dataset (grayscale images of 28 x 28 pixels, of different clothing categories).  
  
![The block architecture]({{ site.baseurl }}/images/SoupNAS_blocks.png)
*The 3-block architecture template*  
  
Each block consists of 10 layers which are shuffled randomly from a pool (later this pool will be optimized by neural architecture search methods):
* size-preserving residual block
* size-preserving convolutional layer
* batch normalization
* RELU
* dropout (p = 0.5)  
  
In the first experiment I played with the stochasticity of the neural network, by defining the input options for each layer. In experiment i in each forward pass, each layer had i options to receive as input, and only one was chosen.  
This means that in experiment #9, a viable block could be 0->1->...->8->9, and also 0->9 as well.  
Experiment #1 defines a regular neural network with no stochasticity while all other experiments introduce randomness in the forward function.


