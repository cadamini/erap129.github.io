---
layout: post
title: Neural Architecture Search for EEG classification
---

In this post I will cover what has been a major part of my thesis, Neural architecture search, and specifically its implication on EEG data classification.

![_config.yml]({{ site.baseurl }}/images/neurons.jpg)

First things's first, what is neural architecture search?  
Deep learning has been gaining major traction in the past couple of years, especially thanks to AlexNet, the CNN architecture created by Krishevsky et al. which dominated the 2012 ImageNet competition, bringing Convolutional neural nets into research spotlight. Since then the best of minds have been creating new and improved deep learning architectures, be it VGGNet or ResNet which re-defined network depth, or GoogLeNet which brought the reusable "inception" module.  
  
Despite all of these improvements, the outcome is always a single neural architecture. The question that should be asked, though, is - "Does a single architecture fit all?", and "Will an architecture that plays well on benchmark datasets work equally well on real-world datasets"?  
 
 ![The AlexNet architecture]({{ site.baseurl }}/images/alexnet.png)
*The seminal AlexNet architecture by Kriscevsky et al.* 

My conjecture to these questions of course is "No!". Why should a single architecture, aimed to reach high benchmark scores, work well on various datasets? For this reason I got into the field of Neural architecture search (NAS). NAS is a means of creating neural network architecture automatically, given a dataset and a specific problem. NAS isn't a new topic and dates back to the late 80's with articles such as [Designing Neural Networks using Genetic Algorithms](https://dl.acm.org/doi/10.5555/93126.94034).
Recent advances in NAS (mainly by [Google](https://arxiv.org/abs/1802.01548)) have brought it to global research focus as well.  
 
Second, what is EEG? Electroencephalography is the action of recording a being's brain waves via electrodes attached to the scalp (externally). These signals are picked up and amplified by special receivers. There is a great interest in classifying these signals accuractely as they can be used for brain computer interfaces (BCI), where a potentially disabled human can contol his/her environment by thought alone.

 ![EEG for BCI]({{ site.baseurl }}/images/eegbci.png)
*The classic approach for BCI signal extraction. In my study CNNs were used for classification, so the "feature extraction" part is built into the network itself by the convolution operations* 
 
In my thesis I decided to combine my interest in Neuroscience and deep learning to create an EEG classifier, tailor made for each dataset. How did I do that? Let's dive in!  
 
----

I chose to implement a simple NAS algorithm, which turned out to work quite well on various EEG datasets. I created a genetic algorithm, which evolves CNN architectures from scratch, and returns the most competent architecture (tested on a hold-out validation set). The architectures evolved by this algorithm are simple series of layers, where layers can be either: Convolution, Dropout, BatchNorm, ELU activation function, MaxPooling or Identity.  

The algorithm runs for 75 generations and has a population size of 100. In each generation the best architectures (tested on a hold-out validation set) are kept with higher probability than bad ones. After this selection, the process of breeding undergoes, where children are created from the surviving architectures to return the population size to 100.  
  
The image below shows the crossover and mutation operations (as part of the genetic algorithm). Two archtiectures are placed one on top of the other and a randomly chosen "cut point" divides both archtictures to two. The resulting child architecture inherits the left side from parent (a) and its right side from parent (b).  

 ![Crossover and mutation in EEGNAS]({{ site.baseurl }}/images/mutation_crossover.png)  
 *Crossover in mutation in EEGNAS*  
   
  The algorithm, dubbed "EEGNAS", worked surprisingly well and managed to create architectures that performed on par or even beat state-of-the-art archiectures hand-designed for classifying these specific EEG datasets.  
  
   ![EEGNAS results]({{ site.baseurl }}/images/eegnas_results.png)  
 *EEGNAS Results*  
   
Given below are the top 5 generated models in the EEGNAS genetic algorithm for dataset 2a of the BCI IV competition. Conv = 1D convolutional layer (F = number of filters, K = kernel size), MaxPool = 1D max pooling layer (K = kernel size, S = stride), [ELU is a non-linear activation function](https://sefiks.com/2018/01/02/elu-as-a-neural-networks-activation-function/). By the low number of convolutional operations we can see that the algorithm preferred simple architectures over complex ones.  
   
 | Layer | Model 1              | Model 2              | Model 3              | Model 4              | Model 5              |
|-------|----------------------|----------------------|----------------------|----------------------|----------------------|
| 1     | BatchNorm            | BatchNorm            | BatchNorm            | Conv   F: 16, K: 12  | Conv   F: 16, K: 12  |
| 2     | ELU                  | ELU                  | ELU                  | BatchNorm            | BatchNorm            |
| 3     | Conv   F: 38, K: 13  | Conv   F: 38, K: 13  | Conv   F: 38, K: 13  | ELU                  | ELU                  |
| 4     | ELU                  | ELU                  | ELU                  | BatchNorm            | BatchNorm            |
| 5     | ELU                  | ELU                  | ELU                  | BatchNorm            | BatchNorm            |
| 6     | ELU                  | ELU                  | ELU                  | ELU                  | ELU                  |
| 7     | MaxPool   K: 1, S: 2 | MaxPool   K: 1, S: 2 | MaxPool   K: 1, S: 2 | ELU                  | ELU                  |
| 8     | MaxPool   K: 1, S: 1 | MaxPool   K: 1, S: 1 | MaxPool   K: 1, S: 1 | MaxPool   K: 1, S: 2 | MaxPool   K: 1, S: 2 |
| 9     | ELU                  | Conv   F: 18, K: 4   | ELU                  | Dropout              | BatchNorm            |
| 10    | Dropout              | ELU                  | Dropout              | BatchNorm            | Conv   F: 18, K: 9   |
| 11    | ELU                  | BatchNorm            | ELU                  | BatchNorm            | Conv   F: 18, K: 4   |
| 12    | Dropout              | BatchNorm            | Dropout              | BatchNorm            | ELU                  |
| 13    | ELU                  | MaxPool   K: 2, S: 1 | ELU                  | ELU                  | BatchNorm            |
| 14    | ELU                  | ELU                  | ELU                  | ELU                  | BatchNorm            |
| 15    | Dropout              | BatchNorm            | Dropout              | ELU                  | ELU                  |
| 16    | Dropout              | Dropout              | Dropout              | ELU                  | BatchNorm            |
| 17    | ELU                  | ELU                  | ELU                  | BatchNorm            | ELU                  |
| 18    | BatchNorm            | ELU                  | BatchNorm            | Dropout              | BatchNorm            |
| 19    | ELU                  | Conv   F: 23, K: 1   | Conv   F: 23, K: 1   | ELU                  | Conv   F: 40, K: 12  |
| 20    | BatchNorm            | Conv   F: 11, K: 12  | Conv   F: 11, K: 13  | ELU                  | ELU                  |
| 21    | Conv   F: 13, K: 6   | ELU                  | ELU                  | Conv   F: 23, K: 1   | ELU                  |
| 22    | Conv   F: 5, K: 109  | Conv   F: 13, K: 1   | ELU                  | Conv   F: 11, K: 12  | Conv   F: 40, K: 11  |
| 23    | Conv   F: 5, K: 99   | Dropout              | ELU                  | ELU                  |
| 24    | Conv   F: 5, K: 102  | Conv   F: 13, K: 1   | ELU                  |
| 25    | Conv   F: 5, K: 104  | Dropout              |
| 26    | Conv   F: 5, K: 83   |
   
 For more information, please read the full research article: <https://link.springer.com/chapter/10.1007/978-981-15-1398-5_1/>, and check out the github repo as well: <https://github.com/erap129/EEGNAS/>

