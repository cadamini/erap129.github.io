---
layout: post
title: Deep learning for time series classification
---

Can deep learning models be explained on non-visual data such as EEG brain scans? In this post I will try to find out.

![_config.yml]({{ site.baseurl }}/images/stock_market.jpg)  

In a previous post I have shown that the EEGNAS algorithm manages to create state of the art level CNN architectures for EEG data classification. Here, I address another research question, which is **why** the network makes certain decisions.  

To this end, I utilized several existing deep learning (DL) interpretability methods, mainly from the image processing field, and also created my own methods.  

**Input frequency perturbations**  
It is known in EEG literature that the alpha (8-12 Hz), beta (12-30Hz) and gamma (30-100+ Hz) wave frequency ranges in EEG signals are good discriminators in motor imagery tasks. I wish to confirm that the CNN architectures generated by EEGNAS use utilize data in these known frequency ranges and the following method helped me achieve this goal.

In a single input-perturbation experiment, consisting of a single EEG dataset and a single \gls{cnn} architecture found by EEGNAS, we first trained the \gls{cnn} on the data for classification accuracy. Then, for each frequency in the range of 1-80Hz we created a new dataset with that frequency range removed and for each of these new datasets we tested the pre-trained network's accuracy.

As shown in the results in below, removing data in the 10Hz (alpha) range impacted classification results the most. Because the alpha range is strongly associated with motor imagery discrimination in the EEG literature, this strengthens the notion that the networks are learning from meaningful data and not noise. Another visible performance dip happens at the 20Hz (beta) range, and a much more subtle one in the gamma range. These also imply that the CNN is learning from meaningful data.

![EEG_perturbations]({{ site.baseurl }}/images/EEG_perturbations.png)  
 *Frequency perturbation experiment results for dataset BCI IV 2b.*  

**Kernel deconvolutions and gradient ascent**  
Using the frequency perturbations technique I learned that the overall network performance is dependent on know frequency ranges, and from this I deducted that it is learning meaningful features. To further assess this assumption I looked into the inner parts of the CNN, which are still a "black box". For this I implemented the [Deconvnet visualization technique by Zeiler and Fergus](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf), originally developed for image data.

This technique allowed me to gain insight into the focus of each learned filter in each convolutional layer. It is done by the use of a "Deconvnet" which is able to use the learned filters of a certain CNN to recreate the input based on the values of a certain filter in a certain activation map along the network. For example, using this method I was able to recreate the input, while highlighting the parts in the input which were important to a certain filter in the network. Because I'm dealing with EEG data, viewing this data isn't straightforward, so I used morlet wavelet convolutions to create comprehensible time-frequency decompositions of the data. These decompositions show us which frequency ranges at which times are important for a certain convolutional filter.  
  
Here we can see the average time-frequency deconvolution of the class "Left hand" for the BCI IV 2b competition dataset. A clear pattern of desynchronization-synchronization is visible in the alpha (10Hz) range.  
![avg_TF]({{ site.baseurl }}/images/bci_iv_2b_desynch.png)  
  
And below we see the result of deconvolution on a filter in the 3rd layer of the CNN generated by EEGNAS. It is clear that this pattern was picked up by the weights in this layer.  
![deconv_TF]({{ site.baseurl }}/images/deconv_EEG.png)  

**Using existing DL interpretability methods for EEG data**  
In progress, coming soon...
